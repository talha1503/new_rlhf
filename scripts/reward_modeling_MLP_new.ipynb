{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15c852a-dff3-4daf-9a95-eeecf57b727c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from source.evaluate_models import evaluate_model\n",
    "from source.losses import preference_loss_function,preference_loss_function_2\n",
    "from source.mlp import MLP\n",
    "from source.training import train_reward_model\n",
    "from source.datasets import TabularDataset\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "880b70ef-72b7-4772-af8e-3d18af856c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input paths\n",
    "synthetic_preferences_path = \"D:\\\\Work\\\\EleutherAI\\\\fairness_gym\\\\ml-fairness-gym\\\\fixed_gpt_preferences_formatted_all.csv\"\n",
    "\n",
    "# Output paths\n",
    "## Fair\n",
    "fair_reward_model_name = f\"fair_reward_model\"\n",
    "fair_training_curve_path_prefix = f\"../data/fair_reward_model_loss\"\n",
    "fair_model_eval_path = f\"../data/fair_reward_model_eval.json\"\n",
    "## Greedy\n",
    "greedy_reward_model_name = f\"greedy_reward_model\"\n",
    "greedy_training_curve_path_prefix = f\"../data/greedy_reward_model_loss\"\n",
    "greedy_model_eval_path = f\"../data/greedy_reward_model_eval.json\"\n",
    "\n",
    "# Schema\n",
    "## For the input dataframe for reward modelling\n",
    "state_action_features = ['default_rate-group_1', 'default_rate-group_2', \n",
    "                         'acceptance_rate-group_1', 'acceptance_rate-group_2', \n",
    "                         'average_credit_score-group_2', 'average_credit_score-group_1',\n",
    "                         'applicant_credit_group',\n",
    "                         'applicant_group_membership',\n",
    "                         'agent_action']\n",
    "\n",
    "target = \"target\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4b58142-041d-4d22-9c22-c0791f82d1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def n_sample_trajectory_df(input_df, n_sample_points = 20):\n",
    "    trajectory_df = input_df.copy()\n",
    "    if len(trajectory_df)//2 > n_sample_points:\n",
    "        sample_step_size = len(trajectory_df) // n_sample_points\n",
    "        trajectory_df = trajectory_df.reset_index(drop=True)\n",
    "        sample_index = [i for i in range(len(trajectory_df)) if i%sample_step_size == 0]\n",
    "        trajectory_df_sampled = trajectory_df[trajectory_df.index.isin(sample_index)]\n",
    "        return trajectory_df_sampled\n",
    "    else:\n",
    "        return trajectory_df\n",
    "\n",
    "def split_fair_greedy_preferences(synthetic_preferences_df):\n",
    "    static_cols = [\"Trajectory_A\", \"Trajectory_B\"]\n",
    "    fair_cols = [i for i in synthetic_preferences_df.columns if 'fair' in i.lower()]\n",
    "    greedy_cols = [i for i in synthetic_preferences_df.columns if 'greedy' in i.lower()]\n",
    "    # Splitting\n",
    "    fair_preferences_df = synthetic_preferences_df[static_cols + fair_cols]\n",
    "    greedy_preferences_df = synthetic_preferences_df[static_cols + greedy_cols]\n",
    "    # Drop NaN\n",
    "    fair_preferences_df = fair_preferences_df.dropna(subset=fair_cols)\n",
    "    greedy_preferences_df = greedy_preferences_df.dropna(subset=greedy_cols)\n",
    "    # Normalize columns naming conventions\n",
    "    fair_preferences_df.columns = [i.strip(\"_fair\").lower() for i in fair_preferences_df.columns]\n",
    "    greedy_preferences_df.columns = [i.strip(\"_greedy\").lower() for i in greedy_preferences_df.columns]\n",
    "    return fair_preferences_df, greedy_preferences_df\n",
    "\n",
    "def extract_trajectory_data(preferences_df, state_action_features, n_sample_points = 20):\n",
    "\n",
    "    preferences_df_formatted = pd.DataFrame()\n",
    "    modelling_df = preferences_df.copy()\n",
    "    for idx, row in tqdm(modelling_df.iterrows(), total=len(modelling_df)):\n",
    "\n",
    "        option_a_file, option_b_file = row['trajectory_a'], row['trajectory_b']\n",
    "        tmp_df_a = pd.read_csv(f\"../data/trajectories/{option_a_file}\", index_col=[0])\n",
    "        tmp_df_b = pd.read_csv(f\"../data/trajectories/{option_b_file}\", index_col=[0])\n",
    "\n",
    "        tmp_df_a_sampled = n_sample_trajectory_df(tmp_df_a, n_sample_points=n_sample_points)\n",
    "        tmp_df_b_sampled = n_sample_trajectory_df(tmp_df_b, n_sample_points=n_sample_points)\n",
    "\n",
    "        tmp_df_a_sampled = tmp_df_a_sampled[state_action_features]\n",
    "        tmp_df_b_sampled = tmp_df_b_sampled[state_action_features]\n",
    "        \n",
    "        tmp_df_a_sampled.columns = [f\"{i}_a\" for i in tmp_df_a_sampled.columns]\n",
    "        tmp_df_b_sampled.columns = [f\"{i}_b\" for i in tmp_df_a_sampled.columns]\n",
    "\n",
    "        tmp_df = pd.concat([tmp_df_a_sampled, tmp_df_b_sampled], axis=1)\n",
    "        tmp_df[\"target\"] = 1 if row['decision'] == \"b\" else 0\n",
    "        preferences_df_formatted = preferences_df_formatted.append(tmp_df)\n",
    "\n",
    "    preferences_df_formatted = preferences_df_formatted.reset_index(drop=True)    \n",
    "    state_action_features_extended = list(preferences_df_formatted.columns)[:-1]\n",
    "    return preferences_df_formatted, state_action_features_extended\n",
    "\n",
    "def preference_loss_function_3(sum_a, sum_b, decisions):\n",
    "    '''\n",
    "    sum_a -> batch_size, 1\n",
    "    sum_b -> batch_size, 1\n",
    "    '''\n",
    "    stacked_tensor = torch.cat([sum_a, sum_b], dim=1)\n",
    "    stacked_tensor = stacked_tensor.to(torch.float32)\n",
    "    decisions = decisions.to(torch.float32)\n",
    "    loss = F.cross_entropy(stacked_tensor, decisions)\n",
    "    return loss\n",
    "    \n",
    "def train_reward_model_wrapper(preferences_df_formatted, \n",
    "                               state_action_features,\n",
    "                               target,\n",
    "                               preference_loss_function,\n",
    "                               model_hidden_config = [64, 64], \n",
    "                               num_epochs=10,\n",
    "                               reward_model_name=\"reward_model\",\n",
    "                               kfold=3,\n",
    "                               batch_size=256,\n",
    "                               model_eval_path=\"data/model_eval_report.json\"\n",
    "                              ):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    target = \"target\"\n",
    "    \n",
    "    X = preferences_df_formatted[state_action_features]\n",
    "    y = preferences_df_formatted[target]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=kfold)\n",
    "\n",
    "    metrics_report_history = {}\n",
    "    losses_history = {}\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y.apply(str))):\n",
    "        print(f\"Fold {i}\")\n",
    "        train_df = preferences_df_formatted[preferences_df_formatted.index.isin(train_index)]\n",
    "        test_df = preferences_df_formatted[preferences_df_formatted.index.isin(test_index)]\n",
    "        X_train, y_train = train_df[state_action_features].to_numpy(), train_df[target].to_numpy()\n",
    "        \n",
    "        # Get dataloader\n",
    "        training_dataset = TabularDataset(\n",
    "            features=X_train, targets=y_train, device=device\n",
    "            )\n",
    "        training_loader = torch.utils.data.DataLoader(\n",
    "                training_dataset, batch_size=batch_size, shuffle=True\n",
    "            )\n",
    "        \n",
    "         \n",
    "        input_dim = len(state_action_features) // 2\n",
    "        reward_model = MLP(name=reward_model_name, \n",
    "                           layer_dims=[X_train.shape[1]//2] + model_hidden_config + [1], \n",
    "                           out_act=None)\n",
    "        \n",
    "        losses = train_reward_model(\n",
    "            reward_model,\n",
    "            input_dim,\n",
    "            training_loader,\n",
    "            loss_function=preference_loss_function_3,\n",
    "            learning_rate=0.0001,\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=256,\n",
    "            save_dir=\"./models/\")\n",
    "        losses_history[i] = losses\n",
    "    \n",
    "        # K-Fold testing\n",
    "        X_test, y_test = test_df[state_action_features].to_numpy(), test_df[target].to_numpy()\n",
    "        metrics_report, cm_df = evaluate_model(reward_model, X_test, y_test)\n",
    "        metrics_report_history[i] = metrics_report\n",
    "\n",
    "    return metrics_report_history, losses_history\n",
    "\n",
    "def get_accuracy_result(metrics_report_history):\n",
    "    acc_list = []\n",
    "    for k, v in metrics_report_history.items():\n",
    "        acc_list.append(v['accuracy'])\n",
    "\n",
    "    print(\"Mean (5-fold): \", np.array(acc_list).mean())\n",
    "    print(\"Std (5-fold): \", np.array(acc_list).std())\n",
    "    return acc_list\n",
    "\n",
    "def display_kfold_metric_report(metrics_report_history):\n",
    "    for k, v in metrics_report_history.items():\n",
    "        print(k)\n",
    "        display(pd.DataFrame(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96f4a7a-2427-4598-b3b0-adc98792644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 16.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# synthetic_preferences_df = pd.read_csv(synthetic_preferences_path).head(100)\n",
    "# fair_preferences_df, greedy_preferences_df = split_fair_greedy_preferences(synthetic_preferences_df)\n",
    "\n",
    "# # fair_preferences_df_formatted = extract_trajectory_data(fair_preferences_df, n_sample_points = 20)\n",
    "# # greedy_preferences_df_formatted = extract_trajectory_data(greedy_preferences_df, n_sample_points = 20)\n",
    "\n",
    "# fair_preferences_df_formatted, state_action_features_extended = extract_trajectory_data(fair_preferences_df, \n",
    "#                                                         state_action_features,\n",
    "#                                                         n_sample_points = 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa7ea439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_a = torch.rand(20,1)\n",
    "# sum_b = torch.rand(20,1)\n",
    "# decisions = torch.rand(20)\n",
    "# decisions = F.one_hot(decisions.to(torch.int64), num_classes=2)\n",
    "\n",
    "# stacked_tensor = torch.cat([sum_a, sum_b], dim=1)\n",
    "# stacked_tensor = stacked_tensor.to(torch.float32)\n",
    "# decisions = decisions.to(torch.float32)\n",
    "# loss = F.cross_entropy(stacked_tensor, decisions)\n",
    "# print(stacked_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53089454-3dd1-4bd3-8c33-d72a08182e6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    }
   ],
   "source": [
    "fair_metrics_report_history, fair_losses_history = train_reward_model_wrapper(fair_preferences_df_formatted, \n",
    "                                                           state_action_features_extended,\n",
    "                                                           target,\n",
    "                                                           preference_loss_function,\n",
    "                                                           num_epochs=5,\n",
    "                                                           model_hidden_config = [32, 32],\n",
    "                                                           reward_model_name=\"reward_model\",\n",
    "                                                           kfold=5,\n",
    "                                                           batch_size = 256\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a37a08cb-455a-4749-8ea3-1f3a3f60b09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0:            precision  recall  f1-score  support  accuracy\n",
      "0               0.28    0.49      0.35    132.0      0.49\n",
      "1               0.59    0.36      0.45    268.0      0.36\n",
      "avg/total       0.44    0.42      0.40    400.0      0.42, 1:            precision  recall  f1-score  support  accuracy\n",
      "0               0.30    0.55      0.39    132.0      0.55\n",
      "1               0.63    0.37      0.47    268.0      0.37\n",
      "avg/total       0.46    0.46      0.43    400.0      0.46, 2:            precision  recall  f1-score  support  accuracy\n",
      "0               0.23    0.45      0.30    132.0      0.45\n",
      "1               0.47    0.24      0.32    268.0      0.24\n",
      "avg/total       0.35    0.34      0.31    400.0      0.34, 3:            precision  recall  f1-score  support  accuracy\n",
      "0               0.33    0.58      0.43    132.0      0.58\n",
      "1               0.68    0.43      0.53    268.0      0.43\n",
      "avg/total       0.50    0.50      0.48    400.0      0.50, 4:            precision  recall  f1-score  support  accuracy\n",
      "0               0.33    0.58      0.42    132.0      0.58\n",
      "1               0.67    0.42      0.52    268.0      0.42\n",
      "avg/total       0.50    0.50      0.47    400.0      0.50}\n"
     ]
    }
   ],
   "source": [
    "print(fair_metrics_report_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d14894-d6b3-41ab-a2d1-cac5759a9ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a50781f8-b780-44f6-adc9-ca8874a79823",
   "metadata": {},
   "source": [
    "***\n",
    "## End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
