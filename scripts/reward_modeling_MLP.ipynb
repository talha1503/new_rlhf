{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d15c852a-dff3-4daf-9a95-eeecf57b727c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "\n",
    "from source.evaluate_models import evaluate_model\n",
    "from source.losses import preference_loss_function, preference_loss_function_2 \n",
    "from source.mlp import MLP\n",
    "from source.training import train_reward_model\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee01722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "a = torch.rand(4,1)\n",
    "b = torch.rand(4,1)\n",
    "\n",
    "c = torch.cat([a,b], dim=1)\n",
    "d = F.softmax(c, dim=1)\n",
    "print(d.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ffa01-ad6d-475b-ac86-981b53c0391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "880b70ef-72b7-4772-af8e-3d18af856c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input paths\n",
    "synthetic_preferences_path = \"D:\\\\Work\\\\EleutherAI\\\\fairness_gym\\\\ml-fairness-gym\\\\fixed_gpt_preferences_formatted_all.csv\"\n",
    "\n",
    "# Output paths\n",
    "## Fair\n",
    "fair_reward_model_name = f\"fair_reward_model\"\n",
    "fair_training_curve_path_prefix = f\"../data/fair_reward_model_loss\"\n",
    "fair_model_eval_path = f\"../data/fair_reward_model_eval.json\"\n",
    "## Greedy\n",
    "greedy_reward_model_name = f\"greedy_reward_model\"\n",
    "greedy_training_curve_path_prefix = f\"../data/greedy_reward_model_loss\"\n",
    "greedy_model_eval_path = f\"../data/greedy_reward_model_eval.json\"\n",
    "\n",
    "# Schema\n",
    "## For the input dataframe for reward modelling\n",
    "state_action_features = ['Trajectory_A', 'Trajectory_B', 'default_rate-group_1', 'default_rate-group_2', \n",
    "                         'acceptance_rate-group_1', 'acceptance_rate-group_2', \n",
    "                         'average_credit_score-group_2', 'average_credit_score-group_1',\n",
    "                         'applicant_credit_group',\n",
    "                         'applicant_group_membership',\n",
    "                         'agent_action']\n",
    "\n",
    "target = \"target\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4b58142-041d-4d22-9c22-c0791f82d1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def n_sample_trajectory_df(input_df, n_sample_points = 20):\n",
    "    trajectory_df = input_df.copy()\n",
    "    if len(trajectory_df)//2 > n_sample_points:\n",
    "        sample_step_size = len(trajectory_df) // n_sample_points\n",
    "        trajectory_df = trajectory_df.reset_index(drop=True)\n",
    "        sample_index = [i for i in range(len(trajectory_df)) if i%sample_step_size == 0]\n",
    "        trajectory_df_sampled = trajectory_df[trajectory_df.index.isin(sample_index)]\n",
    "        return trajectory_df_sampled\n",
    "    else:\n",
    "        return trajectory_df\n",
    "\n",
    "def split_fair_greedy_preferences(synthetic_preferences_df):\n",
    "    static_cols = [\"Trajectory_A\", \"Trajectory_B\"]\n",
    "    fair_cols = [i for i in synthetic_preferences_df.columns if 'fair' in i.lower()]\n",
    "    greedy_cols = [i for i in synthetic_preferences_df.columns if 'greedy' in i.lower()]\n",
    "    # Splitting\n",
    "    fair_preferences_df = synthetic_preferences_df[static_cols + fair_cols]\n",
    "    greedy_preferences_df = synthetic_preferences_df[static_cols + greedy_cols]\n",
    "    # Drop NaN\n",
    "    fair_preferences_df = fair_preferences_df.dropna(subset=fair_cols)\n",
    "    greedy_preferences_df = greedy_preferences_df.dropna(subset=greedy_cols)\n",
    "    # Normalize columns naming conventions\n",
    "    fair_preferences_df.columns = [i.strip(\"_fair\").lower() for i in fair_preferences_df.columns]\n",
    "    greedy_preferences_df.columns = [i.strip(\"_greedy\").lower() for i in greedy_preferences_df.columns]\n",
    "    return fair_preferences_df, greedy_preferences_df\n",
    "    \n",
    "def extract_trajectory_data(preferences_df, n_sample_points = 20):\n",
    "\n",
    "    preferences_df_formatted = pd.DataFrame()\n",
    "    modelling_df = preferences_df.copy()\n",
    "    for idx, row in tqdm(modelling_df.iterrows(), total=len(modelling_df)):\n",
    "\n",
    "        option_a_file, option_b_file = row['trajectory_a'], row['trajectory_b']\n",
    "        tmp_df_a = pd.read_csv(f\"../data/trajectories/{option_a_file}\", index_col=[0])\n",
    "        tmp_df_b = pd.read_csv(f\"../data/trajectories/{option_b_file}\", index_col=[0])\n",
    "\n",
    "        tmp_df_a_sampled = n_sample_trajectory_df(tmp_df_a, n_sample_points=n_sample_points)\n",
    "        tmp_df_b_sampled = n_sample_trajectory_df(tmp_df_b, n_sample_points=n_sample_points)\n",
    "\n",
    "        tmp_df_a_sampled = tmp_df_a_sampled[state_action_features]\n",
    "        tmp_df_a_sampled[\"target\"] = 1 if row['decision'] == \"a\" else 0\n",
    "\n",
    "        tmp_df_b_sampled = tmp_df_b_sampled[state_action_features]\n",
    "        tmp_df_b_sampled[\"target\"] = 1 if row['decision'] == \"b\" else 0\n",
    "\n",
    "        tmp_df = pd.concat([tmp_df_a_sampled, tmp_df_b_sampled], axis=0)\n",
    "        preferences_df_formatted = preferences_df_formatted.append(tmp_df)\n",
    "\n",
    "    preferences_df_formatted = preferences_df_formatted.reset_index(drop=True)    \n",
    "    return preferences_df_formatted\n",
    "\n",
    "def get_trajectories_comparison_df(df,\n",
    "                                  state_action_features,\n",
    "                                  target,\n",
    "                                  trajecotry_folder=\"../data/trajectories\", \n",
    "                                  n_sample_points=20\n",
    "                                 ):\n",
    "    res_df = pd.DataFrame()\n",
    "    modelling_df = df.copy()\n",
    "    for idx, row in tqdm(modelling_df.iterrows(), total=len(modelling_df)):\n",
    "        print(row)\n",
    "        option_a_file = row['Trajectory_A']\n",
    "        option_b_file = row['Trajectory_B']\n",
    "        tmp_df_a = pd.read_csv(f\"{trajecotry_folder}/{option_a_file}\", index_col=[0])\n",
    "        tmp_df_b = pd.read_csv(f\"{trajecotry_folder}/{option_b_file}\", index_col=[0])\n",
    "\n",
    "        tmp_df_a_sampled = n_sample_trajectory_df(tmp_df_a, n_sample_points=n_sample_points)\n",
    "        tmp_df_b_sampled = n_sample_trajectory_df(tmp_df_b, n_sample_points=n_sample_points)    \n",
    "        tmp_df_a_sampled = tmp_df_a_sampled[state_action_features]\n",
    "        tmp_df_b_sampled = tmp_df_b_sampled[state_action_features]\n",
    "\n",
    "        tmp_df_a_sampled.columns = [f\"{i}_a\" for i in tmp_df_a_sampled.columns]\n",
    "        tmp_df_b_sampled.columns = [f\"{i}_b\" for i in tmp_df_b_sampled.columns]\n",
    "        tmp_df = pd.concat([tmp_df_a_sampled, tmp_df_b_sampled], axis=1)\n",
    "\n",
    "        tmp_df[target] = 0 if row[target] == 'a' else 1  \n",
    "        res_df = res_df.append(tmp_df)\n",
    "    res_df = res_df.reset_index(drop=True)\n",
    "    return res_df\n",
    "    \n",
    "def train_reward_model_wrapper(preferences_df_formatted, \n",
    "                               state_action_features,\n",
    "                               target,\n",
    "                               preference_loss_function,\n",
    "                               model_hidden_config = [64, 64], \n",
    "                               num_epochs=10,\n",
    "                               reward_model_name=\"reward_model\",\n",
    "                               kfold=3,\n",
    "                               model_eval_path=\"./model_eval_report.json\"\n",
    "                              ):\n",
    "    target = \"target\"\n",
    "    X = preferences_df_formatted[state_action_features]\n",
    "    y = preferences_df_formatted[target]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=kfold)\n",
    "\n",
    "    metrics_report_history = {}\n",
    "    losses_history = {}\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y.apply(str))):\n",
    "        print(f\"Fold {i}\")\n",
    "        train_df = preferences_df_formatted[preferences_df_formatted.index.isin(train_index)]\n",
    "        test_df = preferences_df_formatted[preferences_df_formatted.index.isin(test_index)]\n",
    "\n",
    "        # Training\n",
    "        X_train, y_train = train_df[state_action_features], train_df[target]\n",
    "        X_test, y_test = test_df[state_action_features], test_df[target]\n",
    "        \n",
    "        train_df = pd.concat([X_train, y_train], axis=1)\n",
    "        test_df = pd.concat([X_test, y_test], axis=1)\n",
    "        # Attach the trajectories to the file names\n",
    "        train_trajectories = get_trajectories_comparison_df(train_df,\n",
    "                                  state_action_features,\n",
    "                                  target=target,\n",
    "                                  trajecotry_folder=\"../data/trajectories\", \n",
    "                                  n_sample_points=20\n",
    "                                 )\n",
    "        test_trajectories = get_trajectories_comparison_df(test_df,\n",
    "                                  state_action_features,\n",
    "                                  target=target,\n",
    "                                  trajecotry_folder=\"../data/trajectories\", \n",
    "                                  n_sample_points=20\n",
    "                                 )\n",
    "        input_dim = (train_trajectories.shape[1]-1)//2\n",
    "        reward_model = MLP(name=reward_model_name, \n",
    "                           layer_dims=[len(state_action_features)+1] + model_hidden_config + [1],\n",
    "                           out_act=None)\n",
    "        y_train = train_trajectories[target].to_numpy()\n",
    "        X_train = train_trajectories.drop(target, axis=1)\n",
    "#         X_train = train_trajectories[state_action_features].to_numpy()\n",
    "        losses = train_reward_model(\n",
    "                    model,\n",
    "                    input_dim,\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    loss_function=preference_loss_function_2,\n",
    "                    learning_rate=0.0001,\n",
    "                    num_epochs=num_epochs,\n",
    "                    batch_size=256,\n",
    "                    save_dir=\"./models/\"\n",
    "                )\n",
    "        losses_history[i] = losses\n",
    "    \n",
    "        # K-Fold testing\n",
    "        \n",
    "#         predictions, metrics_report = evaluate_model(reward_model, test_features, test_decisions)\n",
    "        y_pred = model.predict(X_test)\n",
    "        # evaluate predictions\n",
    "        confusion_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "        metrics_report = metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "        metrics_report[\"confusion_matrix\"] = str(confusion_mat)\n",
    "        metrics_report_history[i] = metrics_report\n",
    "\n",
    "    # the json file where the output must be stored\n",
    "    with open(model_eval_path, \"w\") as f:\n",
    "        json.dump(metrics_report_history, f)\n",
    "\n",
    "    return metrics_report_history, losses_history\n",
    "\n",
    "def get_accuracy_result(metrics_report_history):\n",
    "    acc_list = []\n",
    "    for k, v in metrics_report_history.items():\n",
    "        acc_list.append(v['accuracy'])\n",
    "\n",
    "    print(\"Mean (5-fold): \", np.array(acc_list).mean())\n",
    "    print(\"Std (5-fold): \", np.array(acc_list).std())\n",
    "    return acc_list\n",
    "\n",
    "def display_kfold_metric_report(metrics_report_history):\n",
    "    for k, v in metrics_report_history.items():\n",
    "        print(k)\n",
    "        display(pd.DataFrame(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b96f4a7a-2427-4598-b3b0-adc98792644f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4838/4838 [03:48<00:00, 21.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4814/4814 [03:46<00:00, 21.24it/s]\n"
     ]
    }
   ],
   "source": [
    "synthetic_preferences_df = pd.read_csv(synthetic_preferences_path)\n",
    "fair_preferences_df, greedy_preferences_df = split_fair_greedy_preferences(synthetic_preferences_df)\n",
    "\n",
    "fair_preferences_df_formatted = extract_trajectory_data(fair_preferences_df, n_sample_points = 20)\n",
    "greedy_preferences_df_formatted = extract_trajectory_data(greedy_preferences_df, n_sample_points = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53089454-3dd1-4bd3-8c33-d72a08182e6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Trajectory_A', 'Trajectory_B'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16612\\2477674396.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                                                \u001b[0mmodel_hidden_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                                                \u001b[0mreward_model_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"reward_model\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                                                                \u001b[0mkfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                                                                               )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16612\\1807332682.py\u001b[0m in \u001b[0;36mtrain_reward_model_wrapper\u001b[1;34m(preferences_df_formatted, state_action_features, target, preference_loss_function, model_hidden_config, num_epochs, reward_model_name, kfold, model_eval_path)\u001b[0m\n\u001b[0;32m     90\u001b[0m                               ):\n\u001b[0;32m     91\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"target\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreferences_df_formatted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_action_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreferences_df_formatted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\eleutherai\\rlhf_fairness\\rlhf-fairness\\env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3462\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3464\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3466\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\eleutherai\\rlhf_fairness\\rlhf-fairness\\env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[1;32md:\\work\\eleutherai\\rlhf_fairness\\rlhf-fairness\\env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Trajectory_A', 'Trajectory_B'] not in index\""
     ]
    }
   ],
   "source": [
    "fair_metrics_report_history, fair_losses_history = train_reward_model_wrapper(fair_preferences_df_formatted, \n",
    "                                                                               state_action_features,\n",
    "                                                                               target,\n",
    "                                                                               preference_loss_function,\n",
    "                                                                               num_epochs=3,\n",
    "                                                                               model_hidden_config = [32, 32],\n",
    "                                                                               reward_model_name=\"reward_model\",\n",
    "                                                                               kfold=2\n",
    "                                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9902917-3de6-4bf0-aaf2-610039bc8d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (5-fold):  0.5093065316246383\n",
      "Std (5-fold):  0.0020721372467961685\n"
     ]
    }
   ],
   "source": [
    "fair_acc_list = get_accuracy_result(fair_metrics_report_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a37a08cb-455a-4749-8ea3-1f3a3f60b09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'0': {'precision': 0.5080645161290323, 'recall': 0.6724137931034483, 'f1-score': 0.5787985865724382, 'support': 48720}, '1': {'precision': 0.5055762081784386, 'recall': 0.33971690258118237, 'f1-score': 0.4063745019920319, 'support': 48040}, 'accuracy': 0.5072343943778421, 'macro avg': {'precision': 0.5068203621537355, 'recall': 0.5060653478423154, 'f1-score': 0.49258654428223503, 'support': 96760}, 'weighted avg': {'precision': 0.5068291056913874, 'recall': 0.5072343943778421, 'f1-score': 0.4931924164273088, 'support': 96760}, 'confusion_matrix': '[[32760 15960]\\n [31720 16320]]'}, 1: {'0': {'precision': 0.5113092341741355, 'recall': 0.6686165845648604, 'f1-score': 0.5794768253742362, 'support': 48720}, '1': {'precision': 0.5115125109678981, 'recall': 0.3519150707743547, 'f1-score': 0.41696365811249086, 'support': 48040}, 'accuracy': 0.5113786688714345, 'macro avg': {'precision': 0.5114108725710168, 'recall': 0.5102658276696075, 'f1-score': 0.49822024174336355, 'support': 96760}, 'weighted avg': {'precision': 0.5114101582871198, 'recall': 0.5113786688714345, 'f1-score': 0.498791288424523, 'support': 96760}, 'confusion_matrix': '[[32575 16145]\\n [31134 16906]]'}}\n"
     ]
    }
   ],
   "source": [
    "print(fair_metrics_report_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb4ccaa-4f51-4815-b780-836a848b9afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\work\\eleutherai\\rlhf_fairness\\rlhf-fairness\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\work\\eleutherai\\rlhf_fairness\\rlhf-fairness\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\work\\eleutherai\\rlhf_fairness\\rlhf-fairness\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    }
   ],
   "source": [
    "greedy_metrics_report_history, greedy_losses_history = train_reward_model_wrapper(greedy_preferences_df_formatted, \n",
    "                                                                               state_action_features,\n",
    "                                                                               target,\n",
    "                                                                               preference_loss_function,\n",
    "                                                                               num_epochs=3,\n",
    "                                                                               model_hidden_config = [32, 32],\n",
    "                                                                               reward_model_name=\"reward_model\",\n",
    "                                                                               kfold=2\n",
    "                                                                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d693bf-706d-4a99-a19a-51e59e3c820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (5-fold):  0.5205130868300789\n",
      "Std (5-fold):  0.00929580390527629\n"
     ]
    }
   ],
   "source": [
    "greedy_acc_list = get_accuracy_result(greedy_metrics_report_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b6c720e-d7ba-47e3-b2df-0c6bec5f06bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'0': {'precision': 0.5298088907353552, 'recall': 1.0, 'f1-score': 0.6926471586665761, 'support': 51010}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 45270}, 'accuracy': 0.5298088907353552, 'macro avg': {'precision': 0.2649044453676776, 'recall': 0.5, 'f1-score': 0.34632357933328806, 'support': 96280}, 'weighted avg': {'precision': 0.28069746070222756, 'recall': 0.5298088907353552, 'f1-score': 0.3669706228041342, 'support': 96280}, 'confusion_matrix': '[[51010     0]\\n [45270     0]]'}, 1: {'0': {'precision': 0.5248584015103839, 'recall': 0.8174867673005293, 'f1-score': 0.6392764065613982, 'support': 51010}, '1': {'precision': 0.44682115270350564, 'recall': 0.16611442456372874, 'f1-score': 0.24219001610305962, 'support': 45270}, 'accuracy': 0.5112172829248026, 'macro avg': {'precision': 0.48583977710694476, 'recall': 0.491800595932129, 'f1-score': 0.44073321133222887, 'support': 96280}, 'weighted avg': {'precision': 0.48816598092991675, 'recall': 0.5112172829248026, 'f1-score': 0.4525699161578981, 'support': 96280}, 'confusion_matrix': '[[41700  9310]\\n [37750  7520]]'}}\n"
     ]
    }
   ],
   "source": [
    "print(greedy_metrics_report_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50781f8-b780-44f6-adc9-ca8874a79823",
   "metadata": {},
   "source": [
    "***\n",
    "## End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
